# DQN‑HRL 기반 마켓메이킹(환전) 시뮬레이션 결과 요약

## 0. 요약
본 시뮬레이션은 고객에게 **수수료 없는 환전 서비스**를 제공하면서도, CU(마켓메이커)가 **강화학습 AI(DQN‑HRL)**를 통해 어떻게 **안정적인 수익**을 창출할 수 있는지 검증한 결과입니다.  
AI는 시장 변동성을 실시간으로 학습하여 **익절가(TP)**와 **리스크 한도(Limit)**를 스스로 조절했고, 최종적으로 **약 1,375만 원**의 수익을 달성했습니다.

---

## 1. 강화학습(DQN‑HRL) 아주 쉽게 이해하기

### 1.1 DQN(Deep Q‑Network)이란?
쉽게 말해 **“시행착오로 돈 버는 법을 배우는 아이”**입니다.  
게임기 앞에서 어떤 버튼을 눌러야 점수(보상)가 올라가는지 반복하며 깨닫는 것과 같습니다.

- **상태(State)**:  
  - 지금 시장이 시끄러운가(변동성)?  
  - 내가 달러를 얼마나 가졌는가(재고)?
- **행동(Action)**:  
  - “수익을 짧게 보고 빨리 팔자”  
  - “한도를 늘려 더 버티자” 같은 의사결정
- **보상(Reward)**:  
  - 돈을 벌면 칭찬(+),  
  - 손실이 나거나 재고가 너무 쌓이면 벌점(−)

### 1.2 왜 “계층적(Hierarchical)” 인가? (HRL)
이 모델은 **‘사장님(Manager)’**과 **‘직원(Worker)’**으로 역할을 나누어 효율을 높였습니다.

- **사장님(Manager: DQN Agent)**:  
  100번에 한 번씩 나타나 시장 판세를 읽고,  
  “지금은 변동성이 크니 익절 목표를 높이고 리스크 한도를 넓혀라!”처럼 **전략 가이드라인**을 하달합니다.
- **직원(Worker: Trading Logic)**:  
  사장님이 정해준 규칙 안에서 실제 고객 주문을 처리하며,  
  초단위로 **익절/손절/시간초과**를 체크하며 실무를 수행합니다.

---

## 2. 시스템 구조 및 알고리즘

### 2.1 AI의 판단 지표(Input Features)
1. **HARCH(변동성)**: 최근 환율이 얼마나 출렁였는가  
2. **ACD(체결 주기)**: 고객 주문이 얼마나 자주 들어오는가  
3. **Inventory(재고 상태)**: 현재 보유 달러가 리스크 한도에 얼마나 근접했는가  

### 2.2 사장님의 전략 카드(Action Map)
AI는 상황에 맞춰 아래 **9가지 카드(액션)** 중 하나를 선택합니다.

- **0~2번 카드**: 수익 극대화 전략(높은 익절가)
- **3~5번 카드**: 균형 전략(중간 익절가)
- **6~8번 카드**: 안전 우선 전략(낮은 익절가, 높은 리스크 한도)

### 2.3 3단계 청산 시스템(직원의 실무)
- **Tier 1 (익절)**: 사장님이 정한 목표 수익이 나면 즉시 은행에 팔아 이익 확정
- **Tier 2 (시간 초과)**: 너무 오래 들고 있으면 위험하므로 일정 시간 후 강제 청산
- **Tier 3 (한도 초과)**: 재고가 리스크 한도를 넘으면 시스템 안정을 위해 즉시 청산

---

## 3. 시뮬레이션 결과 분석(Backtest Results)

- **파일명**: `dqn_hrl_simulation_trades_20260222_185201.csv`

### 3.1 최종 수익 현황

| 항목 | 금액(KRW) | 비중 |
| --- | ---:| ---:|
| **최종 실현 손익** | **13,751,682** | **100%** |
| 넷팅(스프레드) 수익 | 9,888,398 | 71.9% |
| 거래(매매) 손익 | 3,863,284 | 28.1% |

> **핵심 인사이트**: 전체 수익의 약 **72%가 넷팅 수익**에서 발생.  
> 이는 AI가 고객 주문을 서로 상쇄시켜 은행 수수료를 아끼는 **마켓메이커 본연의 역할**을 매우 훌륭히 수행했음을 의미합니다.

### 3.2 운영 통계
- **총 이벤트 수**: 25,032건  
- **Tier 1(익절) 성공률**: **74.7%**  
- **최종 인벤토리**: -27,207 USD (리스크 한도 내 안정적 관리)

---

## 4. 기술적 고찰(Technical Insights)

### 4.1 AI는 무엇을 배웠는가?
1. **재고 페널티 인식**:  
   재고가 커질수록 보상에서 감점이 발생. 이를 피하기 위해  
   - 재고가 쌓일 때는 리스크 한도를 넓히는 액션을 선택하거나,  
   - 익절가를 낮춰 빠르게 포지션을 줄이는 방식으로 학습
2. **시간대별 적응**:  
   은행 스프레드가 넓은 시간대에는 더 신중하게 목표가를 설정하여 손실을 방어

### 4.2 소스코드 핵심 로직(Python)

```python
# 사장님(Manager)의 보상 함수: 수익은 높이고 재고 부담은 줄인다!
reward = (current_trade_pnl + netting_profit) - (abs(inventory) * 0.00005)

# 직원(Worker)의 청산 조건 체크
if expected >= o['S_i']:
    method = "Tier1_TP"       # 목표 수익 달성
elif time_diff >= o['T_w']:
    method = "Tier2_TimeOut"  # 너무 오래 들고 있음
elif abs(inventory) > o['Limit']:
    method = "Tier3_RiskLimit" # 재고가 너무 많음
```

---

## 5. 결론 및 향후 계획
DQN‑HRL 모델은 고정 규칙 기반 매매 대비 시장 변화에 더 유연하게 대응하며, **74.7%의 높은 승률**을 기록했습니다.

- **성과**: 넷팅 수익 + 매매 수익의 조화로 안정적인 우상향 곡선 구축
- **향후 과제**
  - 전략 카드(현재 9개)를 **20개 이상**으로 세분화
  - 실제 거래 시 발생하는 **슬리피지(Slippage)**를 더 엄격하게 반영
